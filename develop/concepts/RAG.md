---
created: 2026-02-01
updated: 2026-02-01
tags: [concept, ai, llm, rag]
status: done
---

# RAG (Retrieval-Augmented Generation)

> LLM이 외부 지식 베이스에서 관련 정보를 검색하여 답변의 정확도와 품질을 높이는 기술 프레임워크입니다.

---

## 1. 정의

RAG는 대규모 언어 모델(LLM)이 답변을 생성할 때, 학습된 데이터에만 의존하지 않고 외부의 신뢰할 수 있는 지식 베이스에서 관련 정보를 검색하여 이를 참조함으로써 답변의 정확도와 품질을 높이는 기술 프레임워크입니다.

쉽게 비유하자면, 시험을 볼 때 머릿속에 외운 지식만으로 답을 쓰는 것이 아니라, 신뢰할 수 있는 '참고서(오픈북)'를 찾아보고 내용을 보완하여 답안을 작성하는 방식과 유사합니다.

---

## 2. 등장 배경 및 필요성

기존 LLM이 가진 구조적인 한계를 극복하기 위해 RAG 기술이 주목받게 되었습니다.

- **최신 정보의 부재:** LLM은 학습 시점 이후의 최신 사건이나 데이터는 알지 못합니다.

- **[[GLOSSARY#할루시네이션|할루시네이션]]:** 모델이 사실이 아닌 정보를 마치 사실인 것처럼 그럴듯하게 생성하는 거짓 답변 문제가 발생합니다.

- **비공개 데이터 학습의 어려움:** 기업 내부의 보안 문서나 고객 데이터 등은 퍼블릭 LLM에 학습되어 있지 않으므로, 이를 기반으로 한 답변이 불가능합니다.

---

## 3. 작동 원리

RAG는 크게 **검색(Retrieval), 증강(Augmentation), 생성(Generation)의 세 단계**로 이루어집니다.

1. **질문 수신 및 검색 (Retrieval):** 사용자가 질문을 입력하면, 시스템은 미리 구축된 외부 데이터베이스(주로 [[GLOSSARY#벡터 데이터베이스|벡터 데이터베이스]])에서 해당 질문과 의미적으로 가장 유사한 문서나 데이터를 검색합니다.

2. **프롬프트 증강 (Augmentation):** 사용자의 원래 질문에 검색된 관련 정보를 결합하여 LLM에 전달할 [[GLOSSARY#프롬프트|프롬프트]]를 재구성(증강)합니다.

3. **답변 생성 (Generation):** LLM은 증강된 프롬프트를 바탕으로, 검색된 사실 정보를 참고하여 답변을 생성합니다.

---

## 4. 장점 및 이점

기업 환경에서 RAG를 구축할 경우 얻을 수 있는 효과는 다음과 같습니다.

- **정확성 및 신뢰성 향상:** 검증된 외부 데이터를 근거로 답변하므로, LLM의 환각 현상을 현저히 줄이고 답변의 근거(출처)를 명시할 수 있습니다.

- **최신 데이터 반영:** 모델 전체를 재학습([[GLOSSARY#파인튜닝|파인튜닝]])할 필요 없이, 외부 데이터베이스의 정보만 업데이트하면 즉시 최신 정보를 반영한 답변이 가능합니다.

- **비용 효율성:** LLM을 직접 재학습시키는 것보다 인프라 및 운영 비용이 훨씬 저렴합니다.

- **데이터 보안 유지:** 기업의 민감한 데이터를 LLM 모델 자체에 학습시키지 않고도, 안전한 내부 검색 시스템과 연동하여 활용할 수 있습니다.

---

## 5. 한계점 및 고려사항

RAG가 많은 이점을 제공하지만, 다음과 같은 한계점도 인지해야 합니다.

- **검색 품질 의존성:** RAG의 답변 품질은 검색 단계의 정확도에 크게 좌우됩니다. 관련 없는 문서가 검색되면 오히려 답변 품질이 저하될 수 있습니다.

- **컨텍스트 윈도우 제한:** LLM의 입력 토큰 제한으로 인해, 검색된 모든 관련 문서를 프롬프트에 포함할 수 없는 경우가 발생합니다.

- **검색-생성 불일치:** 검색된 정보가 있음에도 LLM이 이를 무시하거나 잘못 해석하여 답변을 생성할 수 있습니다.

- **실시간 데이터 동기화:** 원본 데이터가 변경될 때마다 [[GLOSSARY#임베딩|임베딩]]을 재생성하고 인덱스를 업데이트해야 하므로, 데이터 파이프라인 관리가 필요합니다.

---

## 6. 핵심 기술 구성 요소

RAG 시스템을 구축하기 위해서는 다음과 같은 기술 요소들이 필요합니다.

- **[[GLOSSARY#임베딩|임베딩]] 모델:** 텍스트를 고차원 벡터로 변환하여 의미적 유사도를 계산할 수 있게 합니다. OpenAI의 text-embedding-ada-002, Sentence-BERT 등이 대표적입니다.

- **[[GLOSSARY#벡터 데이터베이스|벡터 데이터베이스]]:** 임베딩된 문서들을 저장하고, 유사도 기반 검색을 수행합니다. Pinecone, Chroma, Weaviate, Milvus, Qdrant 등이 주요 솔루션입니다.

- **[[GLOSSARY#청킹|청킹]] 전략:** 원본 문서를 적절한 크기의 조각으로 분할하는 방법입니다. 청크 크기, 오버랩 설정, 의미 단위 분할 등이 검색 품질에 직접적인 영향을 미칩니다.

- **[[GLOSSARY#리랭킹|리랭킹]]:** 초기 검색 결과를 재정렬하여 가장 관련성 높은 문서를 선별합니다. Cross-encoder 모델 등을 활용합니다.

---

## 7. RAG vs Fine-tuning 비교

RAG와 [[GLOSSARY#파인튜닝|파인튜닝]]은 LLM을 특정 도메인에 맞게 활용하기 위한 대표적인 두 가지 접근법입니다.

| 구분 | RAG | Fine-tuning |
|------|-----|-------------|
| **적합한 상황** | 최신 정보 반영, 사실 기반 답변, 출처 명시 필요 시 | 특정 스타일/톤 학습, 도메인 특화 언어 패턴 학습 |
| **데이터 업데이트** | 외부 DB 업데이트로 즉시 반영 | 모델 재학습 필요 |
| **비용** | 상대적으로 저렴 (인프라 비용 중심) | 학습 비용 및 시간 소요 |
| **환각 감소** | 검색된 근거 기반으로 효과적 | 학습 데이터 품질에 의존 |
| **구현 복잡도** | 검색 파이프라인 구축 필요 | 학습 데이터 준비 및 학습 과정 필요 |

실무에서는 두 기법을 결합하여 사용하는 경우도 많습니다. 파인튜닝으로 도메인 특화 언어를 학습시킨 모델에 RAG를 적용하여 최신 정보와 사실 기반 답변을 보완하는 방식입니다.

---

## 관련 문서

- [[GraphRAG]] - 지식 그래프를 활용한 RAG 확장 기법
- [[Docker]] - 컨테이너 기반 인프라 구축 시 RAG 시스템 배포에 활용

---

## 참고 자료

- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - RAG 원논문
